# 📘 AI 논문 리뷰 포트폴리오

---

## ✏️ Word2Vec (2013, Mikolov)
📌 **핵심 개념**  
- 단어를 벡터 공간에 임베딩하여 의미적 유사성을 수치로 표현  

📌 **문제 정의**  
- One-hot 벡터는 단어 간 의미 관계를 반영하지 못함  

📌 **제안 방법**  
- Skip-gram, CBOW 모델로 주변 단어 맥락 학습  

📌 **결과**  
- 유사 단어 벡터 근접  
- `"king - man + woman = queen"` 같은 벡터 연산 가능  

📌 **의의**  
- 이후 NLP 임베딩 연구 및 딥러닝 발전의 기초  

---

## ✏️ Seq2Seq (2014, Sutskever)
📌 **핵심 개념**  
- Encoder-Decoder 구조로 입력 시퀀스를 다른 시퀀스로 변환  

📌 **문제 정의**  
- 전통 번역 모델은 긴 문맥과 단어 순서 학습에 한계  

📌 **제안 방법**  
- LSTM Encoder가 입력을 요약 → Decoder가 출력 시퀀스 생성  

📌 **결과**  
- 기계 번역 등 자연어 생성 성능 향상  

📌 **의의**  
- Attention/Transformer 탄생의 기반  

---

## 📖 Attention Is All You Need (2017, Vaswani)
📌 **핵심 개념**  
- Transformer 제안, RNN/CNN을 대체  
- Self-Attention으로 문맥 처리  

📌 **문제 정의**  
- RNN: 긴 문맥 처리 한계, 병렬화 어려움  
- CNN: 국소적 특징엔 강하나 문맥 전체 파악은 약함  

📌 **제안 방법**  
- Encoder-Decoder 구조 + Multi-head Self Attention  
- Positional Encoding으로 순서 정보 보강  

📌 **결과**  
- 번역 과제에서 SOTA 달성  
- 이후 GPT·BERT 등 모든 LLM의 토대가 됨  

📌 **한계 / 의의**  
- 당시 연산 자원 부족 → 대규모 학습 불가  
- 하지만 현대 LLM 혁신의 뼈대  

---

## 📖 BERT (2018, Devlin)
📌 **핵심 개념**  
- Bidirectional Encoder Representations로 문맥 이해 강화  

📌 **문제 정의**  
- 기존 LM은 좌→우, 우→좌 단방향만 가능  

📌 **제안 방법**  
- Masked LM + Next Sentence Prediction  

📌 **결과**  
- GLUE 등 다수 NLP 벤치마크에서 최고 성능  

📌 **의의**  
- 파인튜닝 기반 NLP 혁신 시작  

---

## 📖 GPT-3 (2020, Brown)
📌 **핵심 개념**  
- 초대규모 Transformer LM (175B)  
- In-Context Learning 능력  

📌 **문제 정의**  
- 소규모 모델은 Zero/Few-shot 학습 한계  

📌 **제안 방법**  
- 거대한 LM + Prompt 기반 Few-shot  

📌 **결과**  
- 다양한 과제에서 범용적 성능 발휘  

📌 **의의**  
- ChatGPT와 LLM 서비스 시대 개막  

---

## 📘 AlexNet (2012, Krizhevsky)
📌 **핵심 개념**  
- 대규모 CNN으로 이미지 분류 성능 혁신  

📌 **문제 정의**  
- 기존 비전 모델은 복잡 이미지 처리에 한계  

📌 **제안 방법**  
- ReLU, Dropout, GPU 병렬 학습  

📌 **결과**  
- ImageNet 대회 압도적 우승  

📌 **의의**  
- 딥러닝 컴퓨터비전 붐의 시작  

---

## 📘 ResNet (2015, He)
📌 **핵심 개념**  
- Residual Connection으로 초깊은 네트워크 학습 가능  

📌 **문제 정의**  
- 네트워크가 깊어질수록 기울기 소실 발생  

📌 **제안 방법**  
- Skip Connection(잔차 연결) 도입  

📌 **결과**  
- ImageNet 우승, 딥러닝 CV 성능 향상  

📌 **의의**  
- 현대 비전모델의 기본 구조  

---

## 📘 CLIP (2021, Radford)
📌 **핵심 개념**  
- 텍스트-이미지 대비학습으로 제로샷 분류 가능  

📌 **문제 정의**  
- 특정 데이터셋에 한정된 비전 모델 한계  

📌 **제안 방법**  
- 대규모 텍스트-이미지 쌍 학습  

📌 **결과**  
- 오픈도메인 제로샷 이미지 분류/검색  

📌 **의의**  
- 멀티모달 AI의 대표적 전환점  

---

## 📘 RAG (2020, Lewis)
📌 **핵심 개념**  
- Retrieval-Augmented Generation  

📌 **문제 정의**  
- LLM 환각(hallucination), 최신성 부족  

📌 **제안 방법**  
- 외부 검색 결과를 LLM 입력으로 주입  

📌 **결과**  
- QA/지식탐색 성능 향상  

📌 **의의**  
- 검색-생성 파이프라인의 표준  

---

## 📘 GraphRAG (2024)
📌 **핵심 개념**  
- 지식 그래프 기반 RAG  

📌 **문제 정의**  
- 긴 맥락·대규모 문서에서 단순 검색 한계  

📌 **제안 방법**  
- 문서를 그래프로 변환 → 요약·검색 결합  

📌 **결과**  
- 복잡 질의 처리 성능 향상  

📌 **의의**  
- 최신 RAG 발전 방향  

---
## 📘How transferable are features in deep neural networks?(2014)

📌 한 줄 요약

자연 이미지로 학습한 딥러닝의 하위층(1~2층) 특징은 매우 일반적이라 전이학습에 잘 통하지만, 중간층은 공적응(co-adaptation) 붕괴, 상위층은 과도한 과제 특이성 때문에 성능이 떨어집니다. 그럼에도 거의 어떤 깊이에서든 사전학습 가중치를 초기화로 사용하고 미세조정(fine-tuning)하면 일반화가 지속적으로 향상됩니다.

🧠 연구 배경

이미지 분류나 자가지도/비지도 과제에서 1층 특징이 가보르 필터/색상 블롭으로 수렴하는 현상은 보편적입니다(일반적 특징).

반면 마지막 층은 데이터셋·과제에 매우 특이적입니다.

질문: “일반→특이 전환은 어느 층에서, 얼마나 점진적으로 일어나는가? 이를 정량화할 수 있는가?”

🎯 기여(Contributions)

층별 전이가능성(Generality) 측정 방법 제안: 특정 층까지 복사/동결(또는 미세조정)하여 타깃 과제 성능으로 일반성/특이성을 정량화.

성능 저하의 두 원인 분리

(i) 특징의 특이성(상층으로 갈수록 기존 과제에 최적화)

(ii) 공적응 붕괴(중간층에서 인접층 뉴런 상호의존성이 깨져 최적화 어려움)

과제 간 거리↑ → 전이성↓ (특히 상층 전이에서 두드러짐)

무작위 하위층 가중치 vs. 사전학습 가중치 비교: 대규모/심층 설정에서는 사전학습 전이가 우월.

놀라운 결과: 거의 어느 깊이에서든 사전학습 가중치로 초기화 후 미세조정하면 일반화 향상이 ‘잔존’(초기 학습의 긍정적 흔적이 남음).

🧪 실험 개요
데이터/모델

ImageNet ILSVRC2012 1000클래스 → 무작위로 A/B(각 500 클래스) 로 분할하여 유사 과제 구성.

비유사 과제: WordNet 계층 사용해 인공물(551) vs 자연물(449) 으로 분할.

8층 CNN(AlexNet 계열), Caffe 레퍼런스 구현.

전이 설정(표기: AnB, BnB, +는 fine-tuning)

baseA, baseB: 각각 A/B만으로 원훈련.

BnB: 층 n 까지 baseB 가중치 복사+동결, 위층 랜덤 초기화 후 B로 재학습(자기 전이, 공적응 붕괴 컨트롤).

AnB: 층 n 까지 baseA 가중치 복사+동결, 위층 랜덤 초기화 후 B로 학습(진짜 전이 성능 측정).

BnB+, AnB+: 위와 동일하되 전 층 학습 허용(미세조정).

📈 핵심 결과(요약)

하위층(1–2층) = 매우 일반적

AnB 성능이 baseB와 거의 동일 → 가보르/색상 블롭 + 2층 조합까지 광범위하게 유효.

중간층(3–5층) = 공적응 붕괴가 지배

BnB(자기 전이)에서도 성능 하락 → 동결된 하위층과 새로 학습되는 상위층 사이의 공적응 상실로 최적화 난이도↑.

BnB+(미세조정)로 회복 가능 → 공적응 붕괴는 미세조정으로 해소.

상위층(6–7층) = 과제 특이성 지배

AnB에서 성능 하락이 더욱 두드러짐(특히 과제가 다를수록) → 상층 특징은 원과제에 특이적.

과제 간 거리 영향

인공물↔자연물 간 전이는 무작위 분할보다 전이성 저하가 더 빠르고 더 크게 나타남.

그래도 “먼 과제 전이 > 무작위 가중치”.

사전학습 초기화 + 미세조정의 잔존 이득

AnB+는 baseB보다 평균 +1.6~2.1%p 일반화 향상(층 5–7 유지 시 더 큼).

학습 스텝이 길어서가 아님(BnB+는 동일 스케줄이나 이득 작음). → 사전학습의 좋은 bias가 남음.

🛠️ 실무 적용 가이드 (전이학습 체크리스트)

데이터가 적다 → 하위~중간층을 동결하고 상위층만 학습.

중간층에서 공적응 붕괴가 있을 수 있으니, 점진적 미세조정(unfreeze) 전략 고려(예: 상위→하위 순).

데이터가 충분/중대 과제 → 전층 미세조정이 일반화 향상에 유리.

과제 유사(둘 다 동물 세부 분류 등) → 상위층 전이도 효과적.

과제 상이(인공물↔자연물, 의료↔일반 객체) → 하위층 위주로 전이, 상층은 재학습.

아키텍처 변경 시 → 무작위 하위층은 대규모/심층에서 급격히 성능 저하. 사전학습 백본 권장.

Warm-start 이득을 노려라: 거의 어떤 깊이의 전이 초기화라도 미세조정 후 일반화 향상이 남는다.

🔍 핵심 개념 정리

일반적 특징(General features): 다양한 데이터/과제에 유효(주로 저층의 에지/색상/텍스처).

특이적 특징(Specific features): 원과제에 최적화(주로 상층의 클래스/개념 수준).

공적응(co-adaptation): 인접층 뉴런들이 상호의존적으로 학습된 상태. 중간에 네트워크를 자르면 이 상호작용이 깨져 최적화가 어려워짐.
